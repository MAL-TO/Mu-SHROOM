{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data/val/mushroom.en-val.v2.jsonl\n",
    "import json\n",
    "import os\n",
    "\n",
    "data_dir = \"data/val\"\n",
    "data_file = \"mushroom.en-val.v2.jsonl\"\n",
    "data_path = os.path.join(data_dir, data_file)\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = data[0]['model_input'] + \" \" + data[0]['model_output_text']\n",
    "input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "\n",
    "# print the probability distribution of all tokens\n",
    "#output = base_model(input_ids, return_dict=True)\n",
    "#print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for i, token in enumerate(tokenizer.convert_ids_to_tokens(input_ids[0])):\n",
    "    if i == 0 or i == len(input_ids[0]) - 1:\n",
    "        continue\n",
    "    # get length of token\n",
    "    print(token)\n",
    "    length = len(token)\n",
    "    sum += length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input), sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map token to the relative bound of the token in the input\n",
    "boundaries = data[0]['hard_labels']\n",
    "boundaries = [(start+len(data[0]['model_input']), end+len(data[0]['model_input'])) for start, end in boundaries]\n",
    "print(boundaries)\n",
    "\n",
    "tokens = []\n",
    "# for each boundary, find the corresponding token\n",
    "for boundary in boundaries:\n",
    "    start, end = boundary\n",
    "    token = input[start:end+1]\n",
    "    print(token)\n",
    "    tokens.append(token)\n",
    "\n",
    "    # append token idx \n",
    "\n",
    "print(len(tokenizer.convert_ids_to_tokens(input_ids[0])), tokenizer.convert_ids_to_tokens(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_evaluate = 20  # 22, 26, 34\n",
    "\n",
    "# get the probability distribution of the token\n",
    "tokenizer.convert_ids_to_tokens(int(input_ids[0][idx_to_evaluate])), tokenizer.convert_ids_to_tokens(int(input_ids[0][idx_to_evaluate+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = tokenizer.convert_ids_to_tokens(int(input_ids[0][idx_to_evaluate]))\n",
    "sentence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mnli_label(sentence_1, sentence_2, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 10 \n",
    "\n",
    "# get the topk tokens\n",
    "print(output.logits[0][idx_to_evaluate].topk(topk).indices.shape)\n",
    "topk_tokens = tokenizer.convert_ids_to_tokens(output.logits[0][idx_to_evaluate].topk(topk).indices)\n",
    "print(topk_tokens)\n",
    "# print predicted token\n",
    "print(tokenizer.convert_ids_to_tokens(int(input_ids[0][idx_to_evaluate+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits = output.logits[0][idx_to_evaluate]\n",
    "# softmax the logits\n",
    "output_logits = output_logits.softmax(dim=0)\n",
    "# get the probability of the predicted token\n",
    "# summ all the probabilities of the topk tokens\n",
    "sum = 0\n",
    "for i in range(10):\n",
    "    token = topk_tokens[i]\n",
    "    idx = tokenizer.convert_tokens_to_ids(token)\n",
    "    prob = output_logits[idx].item()\n",
    "    print(token, prob)\n",
    "\n",
    "    \n",
    "    sum += prob\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/nli-deberta-v3-xsmall\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/nli-deberta-v3-xsmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an nli prediction\n",
    "sentence_1 = \"gold\"\n",
    "sentence_2 = \"silver\"\n",
    "\n",
    "# encode the sentences\n",
    "def get_mnli_label(sentence_1, sentence_2, model, tokenizer):\n",
    "    inputs = tokenizer(sentence_1, sentence_2, return_tensors=\"pt\")\n",
    "    # make a prediction\n",
    "    outputs = model(**inputs)\n",
    "    # get the predicted class\n",
    "    predicted_class_idx = outputs.logits.argmax().item()\n",
    "    # get the predicted class name\n",
    "    predicted_class_name = model.config.id2label[predicted_class_idx]\n",
    "    return predicted_class_name\n",
    "\n",
    "get_mnli_label(sentence_1, sentence_2, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input)\n",
    "from termcolor import colored\n",
    "import itertools\n",
    "print(\" \" * len(data[0]['model_input']))\n",
    "\n",
    "def print_colored_text(text):\n",
    "    colors = ['red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white']\n",
    "    color_cycle = itertools.cycle(colors)\n",
    "    \n",
    "    for i in range(0, len(text), 10):\n",
    "        color = next(color_cycle)\n",
    "        print(colored(text[i:i+10], color), end='')\n",
    "    print()\n",
    "\n",
    "# Esempio di utilizzo\n",
    "text = \" \" * (len(data[0]['model_input'])+1) + \"1234567890\" * 9\n",
    "print_colored_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.logits[0][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(output.logits.shape[1]):\n",
    "    print(i)\n",
    "    print(input_ids[0][i], tokenizer.decode(input_ids[0][i].item()), output.logits[0, i].argmax().item(), tokenizer.decode(output.logits[0][i].argmax().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 27\n",
    "output.logits[0][index]\n",
    "\n",
    "# get the top k tokens \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "index = 27\n",
    "top_k = 5\n",
    "values, indices = torch.topk(F.softmax(output.logits[0][index], dim=0), top_k)\n",
    "print(values, indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the top k tokens\n",
    "for i in range(top_k):\n",
    "    print(tokenizer.decode(indices[i].item()), values[i].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
