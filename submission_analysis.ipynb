{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "import argparse as ap\n",
    "\n",
    "def recompute_hard_labels(soft_labels):\n",
    "    \"\"\"optionally, infer hard labels from the soft labels provided\"\"\"\n",
    "    hard_labels = [] \n",
    "    prev_end = -1\n",
    "    for start, end in (\n",
    "        (lbl['start'], lbl['end']) \n",
    "        for lbl in sorted(soft_labels, key=lambda span: (span['start'], span['end']))\n",
    "        if lbl['prob'] > 0.5\n",
    "    ):\n",
    "        if start == prev_end:\n",
    "            hard_labels[-1][-1] = end\n",
    "        else:\n",
    "            hard_labels.append([start, end])\n",
    "        prev_end = end\n",
    "    return hard_labels\n",
    "\n",
    "\n",
    "def infer_soft_labels(hard_labels):\n",
    "    \"\"\"reformat hard labels into soft labels with prob 1\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'prob': 1.0,\n",
    "        }\n",
    "        for start, end in hard_labels\n",
    "    ]\n",
    "\n",
    "def load_jsonl_file_to_records(filename, is_ref=True):\n",
    "    \"\"\"read data from a JSONL file and format that as a `pandas.DataFrame`.\n",
    "    Performs minor format checks (ensures that some labels are present,\n",
    "    optionally compute missing labels on the fly).\"\"\"\n",
    "    df = pd.read_json(filename, lines=True)\n",
    "    if not is_ref:\n",
    "        assert ('hard_labels' in df.columns) or ('soft_labels' in df.columns), \\\n",
    "            f'File {filename} contains no predicted label!'\n",
    "        if 'hard_labels' not in df.columns:\n",
    "            df['hard_labels'] = df.soft_labels.apply(recompute_hard_labels)\n",
    "        elif 'soft_labels' not in df.columns:\n",
    "            df['soft_labels'] = df.hard_labels.apply(infer_soft_labels)\n",
    "    # adding an extra column for convenience\n",
    "    columns = ['id', 'soft_labels', 'hard_labels']\n",
    "    if is_ref:\n",
    "        df['text_len'] = df.model_output_text.apply(len)\n",
    "        columns += ['text_len']\n",
    "    df = df[columns]\n",
    "    return df.sort_values('id').to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_soft_labels(words, hallucination_scores):\n",
    "    soft_labels = []\n",
    "    \n",
    "    # Initialize the starting position of the first word\n",
    "    start_position = 0\n",
    "\n",
    "    for word, score in zip(words, hallucination_scores):\n",
    "        word_length = len(word)\n",
    "        \n",
    "        # Calculate the ending position\n",
    "        end_position = start_position + word_length\n",
    "        \n",
    "        # Append the soft label entry\n",
    "        soft_labels.append({\n",
    "            \"start\": start_position,\n",
    "            \"prob\": score,\n",
    "            \"end\": end_position\n",
    "        })\n",
    "        \n",
    "        # Update the starting position for the next word (accounting for space)\n",
    "        start_position = end_position  # Add 2 for the space between words\n",
    "\n",
    "    return soft_labels\n",
    "\n",
    "# Example usage\n",
    "data = {\n",
    "    \"words evaluated\": [\" \", \"No,\", \"Albero\", \"Foulois\", \"was\", \"not\", \"in\", \"any\", \"of\", \"the\", \"FIFA\", \"World\", \"Cup\", \"finals.\\n\"],\n",
    "    \"hallucination_scores_evaluated\": [0, 1.0, 0.06837508948018978, 0.9810742402775567, 0.5219719747190859, 0.8440420620456921, 0.039483340157654756, 0.7283849860862854, 0.0, 0.12374613816374336, 0.9193932406560029, 0.0, 0.0, 0.0]\n",
    "}\n",
    "\n",
    "soft_labels = calculate_soft_labels(data[\"words evaluated\"], data[\"hallucination_scores_evaluated\"])\n",
    "\n",
    "# Output the result\n",
    "print(soft_labels)\n",
    "\n",
    "hard_labels = recompute_hard_labels(soft_labels)\n",
    "print(hard_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of soft and hard labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "\n",
    "data_dir = \"data/test\"\n",
    "output_path = os.path.join(data_dir, \"results_full.jsonl\")\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\") as f:\n",
    "        processed_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(processed_data[1]['results']), len(processed_data[1]['words_evaluated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "\n",
    "data_dir = \"data/test\"\n",
    "output_path = os.path.join(data_dir, \"results_mult.jsonl\")\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\") as f:\n",
    "        processed_data = [json.loads(line) for line in f]\n",
    "result_path = os.path.join(data_dir, \"final_results.jsonl\")\n",
    "\n",
    "for entry in processed_data: \n",
    "    try:\n",
    "        words_evaluated = entry['words evaluated']\n",
    "        hallucination_scores_evaluated = entry['hallucination_scores_evaluated']\n",
    "    except:\n",
    "        words_evaluated = entry['hallucination_scores_evaluated'][0]\n",
    "        hallucination_scores_evaluated = entry['hallucination_scores_evaluated'][1]\n",
    "    \n",
    "    # if the first element of the words evaluated is \"\", then remove it and the corresponding hallucination score\n",
    "    if words_evaluated[0] == \"\":\n",
    "        words_evaluated[0] = \" \"\n",
    "        hallucination_scores_evaluated[0] = 0\n",
    "    \n",
    "    soft_labels = calculate_soft_labels(words_evaluated, hallucination_scores_evaluated)\n",
    "    hard_labels = recompute_hard_labels(soft_labels)\n",
    "\n",
    "    # save the hard labels to the processed data\n",
    "    entry['hard_labels'] = hard_labels\n",
    "    entry['soft_labels'] = soft_labels\n",
    "\n",
    "    # save the processed data to the new file\n",
    "    with open(result_path, \"w\") as f:\n",
    "        for entry in processed_data:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "        \n",
    "    assert len(words_evaluated) == len(hallucination_scores_evaluated)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNLI on full sentence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data_dir = \"data/test\"\n",
    "output_path = os.path.join(data_dir, \"results_full.jsonl\")\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\") as f:\n",
    "        processed_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/malto/mushroom/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data1/malto/mushroom/.conda/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def get_mnli_probs(sentence_1, sentence_2, model, tokenizer, word_evaluated, device = \"cpu\"):\n",
    "    # if the two sentences are longer than the maximum length, cut them around a defined word \n",
    "\n",
    "    inputs = tokenizer(sentence_1, sentence_2, return_tensors=\"pt\").to(device)\n",
    "    max_length = 512\n",
    "    if len(inputs.input_ids[0]) > max_length:\n",
    "        input_1 = mnli_tokenizer(sentence_1, return_tensors=\"pt\").to(device)\n",
    "        input_2 = mnli_tokenizer(sentence_2, return_tensors=\"pt\").to(device)\n",
    "        word_tokenized = mnli_tokenizer.encode(word_evaluated)[1]\n",
    "        for i, token in enumerate(inputs['input_ids'][0]):\n",
    "            if token == word_tokenized:\n",
    "                token_position = i\n",
    "                break\n",
    "\n",
    "        # cut the sentences around the word\n",
    "        half_length = max_length // 2\n",
    "        start = max(0, token_position - half_length)\n",
    "        end = min(len(inputs['input_ids'][0]), token_position + half_length)\n",
    "\n",
    "        input_1 = input_1['input_ids'][0][start:end]\n",
    "        input_2 = input_2['input_ids'][0][start:end]\n",
    "        input_1_decoded = mnli_tokenizer.decode(input_1, skip_special_tokens=True)\n",
    "        input_2_decoded = mnli_tokenizer.decode(input_2, skip_special_tokens=True)\n",
    "        inputs = mnli_tokenizer(input_1_decoded, input_2_decoded, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    predicted_class_prob = outputs.logits.softmax(dim=1)\n",
    "    return predicted_class_prob\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "mnli_model = AutoModelForSequenceClassification.from_pretrained(\"cross-encoder/nli-deberta-v3-large\").to(device)\n",
    "mnli_tokenizer = AutoTokenizer.from_pretrained(\"cross-encoder/nli-deberta-v3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 43/154 [01:32<03:07,  1.69s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 154/154 [20:51<00:00,  8.13s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for sample in tqdm(processed_data):\n",
    "    try: \n",
    "        device = \"cuda\"\n",
    "        mnli_model = mnli_model.to(device)\n",
    "        gold_sentence = sample['model_output_text']\n",
    "        for i, results in enumerate(sample['results']):\n",
    "            word_to_replace = sample['words_evaluated'][i]\n",
    "            for result in results: \n",
    "                word = result['full_word']\n",
    "                output_sentence = gold_sentence.replace(word_to_replace, word)\n",
    "                prob = get_mnli_probs(gold_sentence, output_sentence, mnli_model, mnli_tokenizer, word_to_replace, device).tolist()\n",
    "                result['sentence_mnli_prob'] = prob\n",
    "            torch.cuda.empty_cache()\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(e)\n",
    "            torch.cuda.empty_cache()\n",
    "            device = \"cpu\"\n",
    "            mnli_model = mnli_model.to(device)\n",
    "            gold_sentence = sample['model_output_text']\n",
    "            for i, results in enumerate(sample['results']):\n",
    "                word_to_replace = sample['words_evaluated'][i]\n",
    "                for result in results: \n",
    "                    word = result['full_word']\n",
    "                    output_sentence = gold_sentence.replace(word_to_replace, word)\n",
    "                    prob = get_mnli_probs(gold_sentence, output_sentence, mnli_model, mnli_tokenizer, device).tolist()\n",
    "                    result['sentence_mnli_prob'] = prob\n",
    "            else: \n",
    "                raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the processed data to the new file\n",
    "result_path = os.path.join(data_dir, \"results_full_sentence.jsonl\")\n",
    "with open(result_path, \"w\") as f:\n",
    "    for entry in processed_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['results'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treshold evaluation on val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_probs_to_array(probs, dim):\n",
    "    array = np.zeros(dim)\n",
    "    \n",
    "    for prob in probs:\n",
    "        start = prob['start']\n",
    "        end = prob['end']\n",
    "        prob = prob['prob']\n",
    "        \n",
    "        array[start:end] = prob\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "data_dir = \"data/val\"\n",
    "output_path = os.path.join(data_dir, \"results_full.jsonl\")\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\") as f:\n",
    "        processed_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = convert_probs_to_array(processed_data[0]['soft_labels'], len(processed_data[0]['model_output_text']))\n",
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in processed_data:\n",
    "    assert len(entry['words_evaluated']) == len(entry['results'])\n",
    "\n",
    "res = processed_data[0]['results']\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\"id2label\": {\n",
    "    \"0\": \"contradiction\",\n",
    "    \"1\": \"entailment\",\n",
    "    \"2\": \"neutral\"\n",
    "  },\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_labels_per_word = []\n",
    "for word in res: \n",
    "    print(word)\n",
    "    p_i = [prob['token_prob'] for prob in word]\n",
    "    print(p_i)\n",
    "    p_plus = [prob['mnli_probs'][0][1] + prob['mnli_probs'][0][2] for prob in word]\n",
    "    print(p_plus)\n",
    "    p_i = np.array(p_i)\n",
    "    p_plus = np.array(p_plus)\n",
    "    hallucination_score = 1 - (sum(p_i * p_plus) / sum(p_i))\n",
    "    print(hallucination_score)\n",
    "    soft_labels_per_word.append(hallucination_score)\n",
    "\n",
    "soft_labels = calculate_soft_labels(processed_data[0]['words_evaluated'], soft_labels_per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_obtained = convert_probs_to_array(soft_labels, len(processed_data[0]['model_output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate mse between two arrays\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(res_obtained, gold)\n",
    "\n",
    "# optimization problem to find a multiplicative factor that minimize the mse\n",
    "\n",
    "thetas = np.linspace(0, 2, 200)\n",
    "mses = []\n",
    "for theta in thetas:\n",
    "    mse = mean_squared_error(res_obtained * theta, gold)\n",
    "    mses.append(mse)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(thetas, mses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "output = []\n",
    "\n",
    "for entry in processed_data:\n",
    "    res = entry['results']\n",
    "    soft_labels_per_word = []\n",
    "    for word in res: \n",
    "        p_i = [prob['token_prob'] for prob in word]\n",
    "        p_plus = [prob['mnli_probs'][0][1] + prob['mnli_probs'][0][2] for prob in word]\n",
    "        #p_plus = [prob['mnli_probs'][0][1] for prob in word]\n",
    "        p_i = np.array(p_i)\n",
    "        p_plus = np.array(p_plus)\n",
    "        hallucination_score = 1 - (sum(p_i * p_plus) / sum(p_i))\n",
    "        soft_labels_per_word.append(hallucination_score)\n",
    "\n",
    "    soft_labels = calculate_soft_labels(entry['words_evaluated'], soft_labels_per_word)\n",
    "    res_obtained = convert_probs_to_array(soft_labels, len(entry['model_output_text']))\n",
    "    gold = convert_probs_to_array(entry['soft_labels'], len(entry['model_output_text']))\n",
    "    target.append(gold)\n",
    "    output.append(res_obtained)\n",
    "\n",
    "print(len(target), len(output))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an optimization problem to find the best theta that minimize the mse\n",
    "thetas = np.linspace(0, 5, 500)\n",
    "mses = []\n",
    "for theta in thetas:\n",
    "    temp = []\n",
    "    for i in range(len(output)):\n",
    "        mse = mean_squared_error(output[i] * theta, target[i])\n",
    "        temp.append(mse)\n",
    "    mses.append(np.mean(temp))\n",
    "\n",
    "plt.plot(thetas, mses)\n",
    "\n",
    "# as title, show the best theta and the mse (2 decimal places)\n",
    "best_theta = thetas[np.argmin(mses)]\n",
    "best_mse = np.min(mses)\n",
    "plt.title(f\"Best theta: {best_theta:.2f}, MSE: {best_mse:.2f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print theta at 1 \n",
    "mses[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a small LLM\n",
    "llm = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "# Define sentences\n",
    "sentence1 = \"He won a gold medal.\"\n",
    "sentence2 = \"He won a silver medal.\"\n",
    "\n",
    "# Craft a minimal prompt\n",
    "prompt = f\"\"\"\n",
    "Determine if the second sentence logically follows from the first.\n",
    "Answer only with \"ENTAILMENT\" or \"NOT ENTAILMENT\".\n",
    "\n",
    "Sentence 1: \"{sentence1}\"\n",
    "Sentence 2: \"{sentence2}\"\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Generate response\n",
    "response = llm(prompt, max_length=10, do_sample=False)\n",
    "label = response[0][\"generated_text\"].strip()\n",
    "\n",
    "print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
