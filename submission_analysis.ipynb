{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "import argparse as ap\n",
    "\n",
    "def recompute_hard_labels(soft_labels):\n",
    "    \"\"\"optionally, infer hard labels from the soft labels provided\"\"\"\n",
    "    hard_labels = [] \n",
    "    prev_end = -1\n",
    "    for start, end in (\n",
    "        (lbl['start'], lbl['end']) \n",
    "        for lbl in sorted(soft_labels, key=lambda span: (span['start'], span['end']))\n",
    "        if lbl['prob'] > 0.5\n",
    "    ):\n",
    "        if start == prev_end:\n",
    "            hard_labels[-1][-1] = end\n",
    "        else:\n",
    "            hard_labels.append([start, end])\n",
    "        prev_end = end\n",
    "    return hard_labels\n",
    "\n",
    "\n",
    "def infer_soft_labels(hard_labels):\n",
    "    \"\"\"reformat hard labels into soft labels with prob 1\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'prob': 1.0,\n",
    "        }\n",
    "        for start, end in hard_labels\n",
    "    ]\n",
    "\n",
    "def load_jsonl_file_to_records(filename, is_ref=True):\n",
    "    \"\"\"read data from a JSONL file and format that as a `pandas.DataFrame`.\n",
    "    Performs minor format checks (ensures that some labels are present,\n",
    "    optionally compute missing labels on the fly).\"\"\"\n",
    "    df = pd.read_json(filename, lines=True)\n",
    "    if not is_ref:\n",
    "        assert ('hard_labels' in df.columns) or ('soft_labels' in df.columns), \\\n",
    "            f'File {filename} contains no predicted label!'\n",
    "        if 'hard_labels' not in df.columns:\n",
    "            df['hard_labels'] = df.soft_labels.apply(recompute_hard_labels)\n",
    "        elif 'soft_labels' not in df.columns:\n",
    "            df['soft_labels'] = df.hard_labels.apply(infer_soft_labels)\n",
    "    # adding an extra column for convenience\n",
    "    columns = ['id', 'soft_labels', 'hard_labels']\n",
    "    if is_ref:\n",
    "        df['text_len'] = df.model_output_text.apply(len)\n",
    "        columns += ['text_len']\n",
    "    df = df[columns]\n",
    "    return df.sort_values('id').to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0, 'prob': 0, 'end': 1}, {'start': 1, 'prob': 1.0, 'end': 4}, {'start': 4, 'prob': 0.06837508948018978, 'end': 10}, {'start': 10, 'prob': 0.9810742402775567, 'end': 17}, {'start': 17, 'prob': 0.5219719747190859, 'end': 20}, {'start': 20, 'prob': 0.8440420620456921, 'end': 23}, {'start': 23, 'prob': 0.039483340157654756, 'end': 25}, {'start': 25, 'prob': 0.7283849860862854, 'end': 28}, {'start': 28, 'prob': 0.0, 'end': 30}, {'start': 30, 'prob': 0.12374613816374336, 'end': 33}, {'start': 33, 'prob': 0.9193932406560029, 'end': 37}, {'start': 37, 'prob': 0.0, 'end': 42}, {'start': 42, 'prob': 0.0, 'end': 45}, {'start': 45, 'prob': 0.0, 'end': 53}]\n",
      "[[1, 4], [10, 23], [25, 28], [33, 37]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_soft_labels(words, hallucination_scores):\n",
    "    soft_labels = []\n",
    "    \n",
    "    # Initialize the starting position of the first word\n",
    "    start_position = 0\n",
    "\n",
    "    for word, score in zip(words, hallucination_scores):\n",
    "        word_length = len(word)\n",
    "        \n",
    "        # Calculate the ending position\n",
    "        end_position = start_position + word_length\n",
    "        \n",
    "        # Append the soft label entry\n",
    "        soft_labels.append({\n",
    "            \"start\": start_position,\n",
    "            \"prob\": score,\n",
    "            \"end\": end_position\n",
    "        })\n",
    "        \n",
    "        # Update the starting position for the next word (accounting for space)\n",
    "        start_position = end_position  # Add 2 for the space between words\n",
    "\n",
    "    return soft_labels\n",
    "\n",
    "# Example usage\n",
    "data = {\n",
    "    \"words evaluated\": [\" \", \"No,\", \"Albero\", \"Foulois\", \"was\", \"not\", \"in\", \"any\", \"of\", \"the\", \"FIFA\", \"World\", \"Cup\", \"finals.\\n\"],\n",
    "    \"hallucination_scores_evaluated\": [0, 1.0, 0.06837508948018978, 0.9810742402775567, 0.5219719747190859, 0.8440420620456921, 0.039483340157654756, 0.7283849860862854, 0.0, 0.12374613816374336, 0.9193932406560029, 0.0, 0.0, 0.0]\n",
    "}\n",
    "\n",
    "soft_labels = calculate_soft_labels(data[\"words evaluated\"], data[\"hallucination_scores_evaluated\"])\n",
    "\n",
    "# Output the result\n",
    "print(soft_labels)\n",
    "\n",
    "hard_labels = recompute_hard_labels(soft_labels)\n",
    "print(hard_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "\n",
    "data_dir = \"data/test\"\n",
    "output_path = os.path.join(data_dir, \"results_full.jsonl\")\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\") as f:\n",
    "        processed_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_data[1]['results']), len(processed_data[1]['words_evaluated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "\n",
    "data_dir = \"data/test\"\n",
    "output_path = os.path.join(data_dir, \"results_mult.jsonl\")\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\") as f:\n",
    "        processed_data = [json.loads(line) for line in f]\n",
    "result_path = os.path.join(data_dir, \"final_results.jsonl\")\n",
    "\n",
    "for entry in processed_data: \n",
    "    try:\n",
    "        words_evaluated = entry['words evaluated']\n",
    "        hallucination_scores_evaluated = entry['hallucination_scores_evaluated']\n",
    "    except:\n",
    "        words_evaluated = entry['hallucination_scores_evaluated'][0]\n",
    "        hallucination_scores_evaluated = entry['hallucination_scores_evaluated'][1]\n",
    "    \n",
    "    # if the first element of the words evaluated is \"\", then remove it and the corresponding hallucination score\n",
    "    if words_evaluated[0] == \"\":\n",
    "        words_evaluated[0] = \" \"\n",
    "        hallucination_scores_evaluated[0] = 0\n",
    "    \n",
    "    soft_labels = calculate_soft_labels(words_evaluated, hallucination_scores_evaluated)\n",
    "    hard_labels = recompute_hard_labels(soft_labels)\n",
    "\n",
    "    # save the hard labels to the processed data\n",
    "    entry['hard_labels'] = hard_labels\n",
    "    entry['soft_labels'] = soft_labels\n",
    "\n",
    "    # save the processed data to the new file\n",
    "    with open(result_path, \"w\") as f:\n",
    "        for entry in processed_data:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "        \n",
    "    assert len(words_evaluated) == len(hallucination_scores_evaluated)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
